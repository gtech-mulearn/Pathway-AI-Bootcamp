# 3.4. Transforming Vectors into LLM Responses | Word Vectors Simplified ğŸ§©ğŸ¤–

Alright, you've got a handle on word vectors and the role of context. ğŸš€ğŸ“š Before we delve into the comprehensive pipelines that make Large Language Models (LLMs) function, it's crucial to understand the roles of tokenizers and detokenizers.

Think of a tokenizer as a "sentence chopper." ğŸª“ It breaks down a sentence into smaller parts, like words, characters, subwords, or symbols, which the model can understand. This generally depends on the type and the size of the model. Detokenizers do the reverse; ğŸ§µ they take the LLM's output and stitch it back into sentences we can understand. This process is a foundational step for LLMs to translate human queries into actionable tasks. ğŸ› ï¸ğŸ’¬

**Up Next: Explainer by Mike Chambers from AWS** ğŸ’¡

To give you a more concrete understanding, our next resource is an insightful video by Mike Chambers. ğŸ¥ This video demystifies what happens when you send a â€˜promptâ€™ to an LLM. Though the internal mathematics may be intricate, the overall goal is straightforward: word prediction. ğŸ¤¯

The video will guide you through how your prompts are processed to generate coherent text responses. This will set the stage for our subsequent discussions on Prompt Engineering and LLM pipelines, offering a cohesive picture of how these models operate. ğŸ¤“ğŸ“Š

ğŸ“º [![Watch the Video](https://img.youtube.com/vi/ibr5wmtinG0?si=uFmxljq3yj1NQHy3/0.jpg)](https://youtu.be/ibr5wmtinG0?si=uFmxljq3yj1NQHy3)

**How LLMs Work | Introduction to Large Language Models (LLMs) (2/6)**

(Credits: Build on AWS) ğŸ‰
