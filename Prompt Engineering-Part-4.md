4.4. Token Limits | Prompt Engineering ğŸš€ğŸ“
By now, you know LLMs are the AI powerhouses trained on heaps of data, and prompts are what enable you to make the most out of them.

However, itâ€™s important to learn that different LLMs have specific token limits that define their performance. Ideally, when youâ€™re creating your prompt, you need to ensure that youâ€™re not crossing these token limits. Letâ€™s understand this concept quickly.

Token Limits: These dictate how many tokens an LLM can handle in one go. ğŸ›ï¸

Estimated Word Counts: This refers to the approximate number of words that can fit within a modelâ€™s token limit. It helps you gauge how much content you can generate or process. ğŸ“Š

If you try copy-pasting a long Wikipedia article (for example, that of Google), youâ€™ll notice an error.

![short pe](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/PE-1%2C4.4.png)

Think of token and word counts as your LLM's capacity. While tokens define the technical limit, estimated word counts translate this into a more human-understandable measure.

Why It Matters: Knowing the estimated word count helps you manage your input prompts and outputs more efficiently. ğŸ§ ğŸ“ˆ

Comparative Analysis: Token and Estimated Word Counts in a Few Leading LLMs ğŸ“Š

![table](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/PE-1%2C4.4%20table.png)

While the foundational knowledge provided is adequate for course progression, further exploration of tokens is available in the documentation linked below.

- [Tokens and Efficient Prompt Design | Open AI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)ğŸ“˜
- [LLM AI Tokens | Microsoft](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens) ğŸ“–

[Next Lesson](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/Task-4.md) ğŸ“–ğŸ‘£ğŸ”œ

[Previous Lesson](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/Prompt%20Engineering%20Part-3.md)ğŸ”™ğŸ“š
