# 3.3. Role of Context | Word Vectors Simplified ğŸŒğŸ¤”

Let's dive a bit deeper into the world of word vectors and explore how context comes into play. 

<div align="center">
<img src="https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/WVS%20ChatGPT.png"height='200'>
</div>

Imagine you're trying to understand the word "apple." Without context, it could be a fruit or a tech company. ğŸğŸ’» But what if I say, "I ate an apple"? Now it's clear, right? Context helps us make sense of words, and it's no different for large language models. ğŸğŸ´

**Technical Explanation Made Simple** ğŸ¤“

In general, large language models like GPT-4 or Llama use various techniques to understand the context surrounding each word. For instance, GPT-4 leverages a popular and efficient technique called the "attention mechanism," which helps the model focus on different parts of the text to understand it better. ğŸ§ ğŸ” However, older models might use other strategies like Recurrent Neural Networks (RNNs) or Long Short-Term Memory Networks (LSTMs) to capture context in a different way. ğŸ“šğŸ’¡
****
