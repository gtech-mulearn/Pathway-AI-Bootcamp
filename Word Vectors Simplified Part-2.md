# 3.2. Word Vector Relationships | Word Vectors Simplified ğŸŒŸğŸ“šğŸ¥

Navigating the landscape of text representation, it's essential to grasp how words relate to each other in vector form. In the upcoming video, Anup Surendran dives into the history of word vectors and takes a closer look at Google's groundbreaking Word2Vec project. Why are vector relationships so critical, and what biases do they bring?

Let's find out!

ğŸ“º [![Watch the Video](https://img.youtube.com/vi/z9TkiaETmEs/0.jpg)](https://youtu.be/z9TkiaETmEs)

Here, Anup traces the evolution of word vectors, emphasizing the milestone that is Google's Word2Vec project. One of the standout features of Word2Vec is vector arithmetic, allowing us to reason about words mathematically. 

For example, the vector equation "King - Man + Woman = Queen" showcases this property brilliantly.

The video also explores the role of word vector relationships in similarity searchâ€”a key capability in large language models. However, Anup goes ahead to discuss a very important component, i.e. the biases that inherently exist in these major technological developments. 

Understanding these relationships and their implications not only deepens our grasp of Large Language Models but also equips us to use them more responsibly. ğŸŒ

ğŸ’¡A practical insight: Youâ€™ll often hear the â€œvector embeddingsâ€ and â€œword vectorsâ€ being used interchangeably in the context of LLMs. As of now, OpenAI's text-embedding-ada-002 is commonly used for generating efficient vector embeddings from various types of structured and unstructured data. These are then stored in vector indexes, specialized data structures engineered to ensure rapid and relevant data access using these embeddings. ğŸš€

[Next Lesson](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/Word%20Vectors%20Simplified.md) ğŸ“–ğŸ‘£ğŸ”œ

[Previous Lesson](https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/Word%20Vectors%20Simplified%20Part-1.md) ğŸ”™ğŸ“š
